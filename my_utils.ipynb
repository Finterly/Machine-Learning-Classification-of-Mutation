{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction\n",
    "def make_prediction(model_name, model1, model2, X_final_1, X_final_2, final_val_dat):\n",
    "    pred_1 = pd.DataFrame(model1.predict(X_final_1.iloc[:,1:8]))\n",
    "    id_1 = pd.DataFrame(X_final_1['id']).reset_index(drop=True)\n",
    "    df_new = pd.concat([id_1, pred_1], axis=1)\n",
    "    pred_2 = pd.DataFrame(model2.predict(X_final_2.iloc[:,1:7]))\n",
    "    id_2 = pd.DataFrame(X_final_2['id']).reset_index(drop=True)\n",
    "    df2 = id_2.join(pred_2)\n",
    "    df_new = df_new.append(df2) \n",
    "    df_new = df_new.set_index('id')\n",
    "    df_new = df_new.reindex(index=final_val_dat['id'])\n",
    "    df_new = df_new.reset_index() \n",
    "    df_new = df_new.rename(columns={0: model_name})\n",
    "    return df_new\n",
    "\n",
    "# Make predictions for certain models: like XGB and Voting Classifier which\n",
    "# do not accept dataframes \n",
    "def make_prediction2(model_name, model1, model2, X_final_1, X_final_2, final_val_dat):\n",
    "    X_final_col1 = X_final_1.drop('id', axis=1).values\n",
    "    X_final_col2 = X_final_2.drop('id', axis=1).values\n",
    "    pred_1 = pd.DataFrame(model1.predict(X_final_col1))\n",
    "    id_1 = pd.DataFrame(X_final_1['id']).reset_index(drop=True)\n",
    "    df_new = pd.concat([id_1, pred_1], axis=1)\n",
    "    pred_2 = pd.DataFrame(model2.predict(X_final_col2))\n",
    "    id_2 = pd.DataFrame(X_final_2['id']).reset_index(drop=True)\n",
    "    df2 = id_2.join(pred_2)\n",
    "    df_new = df_new.append(df2) \n",
    "    df_new = df_new.set_index('id')\n",
    "    df_new = df_new.reindex(index=final_val_dat['id'])\n",
    "    df_new = df_new.reset_index()\n",
    "    df_new = df_new.rename(columns={0: model_name})\n",
    "    return df_new\n",
    "\n",
    "# Function for plotting ROC and Precision Recall curve\n",
    "def plot_roc_and_pr_curves (model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_prob = model.predict_proba(X_test)[:,1] #prob for positive outcome only\n",
    "    model_precision, model_recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 3))  \n",
    "    ax[0].plot([0, 1], [0, 1], 'k--')\n",
    "    ax[0].plot(fpr, tpr)   #ROC curve\n",
    "    ax[0].set_xlabel('False Positive Rate')\n",
    "    ax[0].set_ylabel('True Positive Rate')\n",
    "    ax[0].set_title('ROC Curve') \n",
    "    ax[1].plot(model_recall, model_precision, label='Logistic') #PR curve\n",
    "    ax[1].set_xlabel('Recall')\n",
    "    ax[1].set_ylabel('Precision')\n",
    "    ax[1].set_title('Precision Recall Curve')\n",
    "\n",
    "\n",
    "# Function for printing metrics\n",
    "def print_model_metrics(model, X_test, y_test, y_pred, model_nm, sum_table, tuning=False, roc_pr=False):\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    av_pr = metrics.average_precision_score(y_test, y_pred)\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(\"Accuracy: \", acc)\n",
    "    print(\"AUC: \", auc)\n",
    "    print(\"Average Precision: \", av_pr)\n",
    "    print(\"f1: \", f1)\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    if roc_pr is True: \n",
    "        plot_roc_and_pr_curves (model, X_test, y_test)\n",
    "    if tuning is True: \n",
    "        best_param = model.best_params_\n",
    "        best_acc = model.best_score_\n",
    "        print(\"Tuned Parameters: \", best_param)\n",
    "        print(\"Best model accuracy: \", best_acc)\n",
    "        sum_table  = sum_table.append({'model': model_nm, 'acc': acc, 'f1':f1, 'auc':auc,\n",
    "                                       'av_pr':av_pr,'best_acc':best_acc},ignore_index=True)\n",
    "    else: \n",
    "        sum_table  = sum_table.append({'model': model_nm, 'acc': acc, 'f1':f1, 'auc':auc,\n",
    "                                       'av_pr':av_pr,'best_acc': acc},ignore_index=True)\n",
    "    return sum_table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
